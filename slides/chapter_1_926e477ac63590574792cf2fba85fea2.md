---
title: Insert title here
key: 926e477ac63590574792cf2fba85fea2

---
## Frequentist A/B Testing
Difference Between Means

```yaml
type: "TitleSlide"
key: "6f6f31381d"
```

`@lower_third`

name: Megan Robertson
title: Data Scientist at Nike


`@script`
Now that you've got an idea of A/B testing works and the different schools of statistical thought, let's go into more detail on frequentist testing. Our first We're going to begin by going over how to conduct a test to determine if there is a difference between the average metrics of groups shown different things.


---
## A Motivating Example

```yaml
type: "FullSlide"
key: "505c958576"
```

`@part1`
Null: No difference is average time spent on site between those shown and not shown the ad

Alternative: Those shown ad spend more time on the site


`@script`
Let's introduce frequentist A/B testing with an example that we can consider throughout the lesson. You are an analyst at an online shopping company that is debating adding a pop-up promotion when users enter the site. 

The marketing team thinks free shipping on the first order, promoted via this pop-up, will make customers stay on the page longer and shop more. However, there is concern that such an ad could annoy customers and cause them to leave the site quickly. 

You set up an A/B test to determine if the site should keep the ad. When users visit the site, there is a 50% chance an ad pops up. The time on site is recorded for each group


---
## Conducting the test

```yaml
type: "FullSlide"
key: "098a0dccc9"
```

`@part1`
H_0 No difference in average amount watched

H_1 People watch more of shorter intro videos


`@script`
- based on the data from A/B test, is there evidence to support the alternative?
- let's look at the data
- assume null is true and see how likely our results are given that assumption
- need a way to measure "how different" the two means are
- this is the test statistic


---
## Conditions for Test

```yaml
type: "FullSlide"
key: "a563c6e1f7"
```

`@part1`
1. Independent and Nearly Normal Observations
   - randomly assign users to group
   - data should follow a bell curve
2. Large sample size
   - at least 30
3. Population distribution is not skewed
   - looking for large outliers


`@script`
- need to check some conditions for the test
   - independent because


---
## A/B Testing in Python

```yaml
type: "FullSlide"
key: "ebc1fde2b0"
```

`@part1`
``
from spicy import stats

test_stat, p_val = scipy.stats.ttest_ind(group_a, group_b, equal_var = False)

print(test_stat)
print(p_val)
``


`@script`
Now that we've covered the basics of A/B testing, we're going to move on to how to conduct the test in Python. Once you've got your data loaded, it only takes a few lines of code. Here we are using a function from the scipy function. The first two arguments are the data from the two different versions of the trial. The third argument is whether or not the two groups have equal variances. The code returns the test statistic and the p-value.


---
## Final Slide

```yaml
type: "FinalSlide"
key: "fd755dd616"
```

`@script`


