---
title: Insert title here
key: 926e477ac63590574792cf2fba85fea2

---
## Frequentist A/B Testing

```yaml
type: "TitleSlide"
key: "6f6f31381d"
assignment: "Difference Between Means"
```

`@lower_third`

name: Megan Robertson
title: Data Scientist at Nike


`@script`
Now that you've got an idea of A/B testing works and the different schools of statistical thought, let's go into more detail on frequentist testing. Our first We're going to begin by going over how to conduct a test to determine if there is a difference between the average metrics of groups shown different things.


---
## A Motivating Example

```yaml
type: "FullSlide"
key: "505c958576"
```

`@part1`
**Null**: No difference is average time spent on site {{1}}    

**Alternative**: Those shown ad spend more time on the site {{2}}

**Significance Level:** \alpha = 0.05 {{3}}


`@script`
Let's introduce frequentist A/B testing with an example that we can consider throughout the lesson. You are an analyst at an online shopping company that is debating adding a pop-up promotion when users enter the site. 

The marketing team thinks free shipping on the first order, promoted via this pop-up, will make customers stay on the page longer and shop more. However, there is concern that such an ad could annoy customers and cause them to leave the site quickly. 

You set up an A/B test to determine if the site should use the ad. When users visit the site, there is a 50% chance an ad pops up. The time on site is recorded for each group 

In this test, we need to determine if the difference in average times between the two groups is large enough to be evidence that the ad improves shoppers' time on the site. In statistics, we refer to each option as a hypothesis. The null hypothesis states that there is no difference in average time spend on the site, and the alternative hypothesis is that those shown the ad spend more time on the site. 

We will assume that there is no difference between the two groups and see how unusual our experiment results are under this assumption, the next slide will more clearly define what we mean by unusual. 

Prior to the test, you select a threshold that determines how unusual your data has to be. This is called the significant level and typically written with an alpha symbol. This value is typically 0.05, but can be adjusted to be more or less conservative.


---
## Math Behind Comparing Means

```yaml
type: "TwoColumns"
key: "7156847fa3"
```

`@part1`
standard error def


test statistic


`@part2`
![](image-url)

picture of t-distribution with upper tail


`@script`
In order to quantify the difference between the two groups,   we'll use a measure we call the test statistic. The test statistic has three parts. One part is the standard error. To calculate standard error, we only need the variance of the data for each group and the number of observations. We use standard error since we are working with averages. 

The two others things we need for the test statistic are the point estimate and null value. Since our null hypothesis is the averages are the same for the two groups, the value is 0. The point estimate is just the difference between the two averages of the groups observed in the A/B test. Dividing the difference between the point estimate from the data and the null value by the standard error measures how far from normal the experiment data is. 

In order to measure this abnormality, we can look at a t distribution to get an idea of how unusual the observation is. Suppose the red vertical line is where our test statistic is on the x axis. In the image, you can see the shaded tail corresponds to the situations more extreme than what was observed in our experiment. This area is called the p-value, and is the probability of observing results like what was seen in the experiment (or something more extreme) given that the null is true. Thus, the smaller the p-value, the less likely the results under the null and the stronger the evidence is in support of the alternative.


---
## Conditions for Test

```yaml
type: "FullSlide"
key: "a563c6e1f7"
```

`@part1`
1. Independent and Nearly Normal Observations
   - randomly assign users to group
   - less than 10% of total population
2. Large sample size
   - at least 30
3. Population distribution is not skewed
   - looking for large outliers


`@script`
In order to conduct the test using the t-distribution as a comparison, a few conditions need to be met. We want to assume that the difference between the two means follows a t-distribution. We need the observations to be independent. This assures that there is no relation between any of the observations that could be accounting for some of the same information.


---
## Conducting the Test in Python

```yaml
type: "FullSlide"
key: "ebc1fde2b0"
```

`@part1`
``
from spicy import stats

test_stat, p_val = scipy.stats.ttest_ind(group_a, group_b, equal_var = False)

print(test_stat)
print(p_val)
``


`@script`
Now that we've covered the basics of A/B testing, we're going to move on to how to conduct the test in Python. Once you've got your data loaded, it only takes a few lines of code. Here we are using a function from the scipy function. The first two arguments are the data from the two different versions of the trial. The group_a and group_b objects are just vectors containing all the observed times on site for each group. The third argument is whether or not the two groups have equal variances. The code returns the test statistic and the p-value.


---
## Final Slide

```yaml
type: "FinalSlide"
key: "fd755dd616"
```

`@script`
Knowing how to test if the difference between two means is significant opens up many possibilities for A/B testing in a business setting. You can use this technique in any situation where you want to compare the average value of a metric between two groups and the conditions for the test are met. You are now able to plan this type of test and execute it in Python.

While comparing means covers a lot situations you might want to test, there are other types of frequentist A/B tests that you might want to use. In the next lesson, we will learn more about comparing proportions between two groups.

