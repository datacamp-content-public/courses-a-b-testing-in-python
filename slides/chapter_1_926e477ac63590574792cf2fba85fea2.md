---
title: Insert title here
key: 926e477ac63590574792cf2fba85fea2

---
## Frequentist A/B Testing

```yaml
type: "TitleSlide"
key: "6f6f31381d"
assignment: "Difference Between Means"
```

`@lower_third`

name: Megan Robertson
title: Data Scientist at Nike


`@script`
Now that you've been introduced to A/B testing, let's go into more detail on frequentist testing. We're going to begin by going over how to conduct a test to determine if there is a difference between the average metrics of data collected from two groups in an A/B test.


---
## A Motivating Example

```yaml
type: "FullSlide"
key: "505c958576"
```

`@part1`
**Null**: No difference is average time spent on site {{1}}    

**Alternative**: Those shown ad spend more time on the site {{2}}


`@script`
Let's introduce frequentist A/B testing with an example that we can consider throughout the lesson. You are an analyst at an online shopping company that is debating adding a pop-up promotion for free shipping on the next order.. 

The marketing team thinks this will make customers stay on the page longer and shop more. However, there is concern that such an ad could annoy customers and cause them to leave the site quickly. 

You set up an A/B test to determine if the site should use the ad. When users visit the site, there is a 50% chance an ad pops up. The time on site is recorded for each group. 

We need to determine if the difference in average times between the two groups is large enough to be evidence that the ad improves shoppers' time on the site. 

In statistics, we refer to each option, pop-up or no pop-up, as a hypothesis. The null hypothesis states that there is no difference in average time spent on the site, and the alternative hypothesis is that those shown the ad do spend more time on the site. 

We will assume that there is no difference between the two groups and see how unusual our A/B test results are under this assumption. The next slide clarify what we mean by unusual.


---
## Math Behind Comparing Means

```yaml
type: "TwoColumns"
key: "7156847fa3"
```

`@part1`
![](http://assets.datacamp.com/production/repositories/3877/datasets/52613184090b898631f88bc0f8965988445b4840/SE_formula.png) {{1}}


![](http://assets.datacamp.com/production/repositories/3877/datasets/7c4fa99d39e88477c625814365fe90dad8f20232/test_stat.png){{2}}


`@part2`
![](http://assets.datacamp.com/production/repositories/3877/datasets/8ca619f43561b6e57c005ab64e4b1fbf9b94cbf9/t_dist.png) {{3}}


`@script`
In order to quantify the difference between the two groups,   we'll use a measure we call the test statistic. The test statistic has three parts. One part is the standard error, denoted SE. To calculate standard error, we only need the variance of the data (s one and s two squared) for each group and the number of observations in each group (n one and n two). We use standard error here since we are working with averages. 

The two others things we need for the test statistic, T,  are the point estimate and null value. Since our null hypothesis is that the averages are the same for the two groups, the value is 0. The point estimate is just the difference between the two averages of the groups observed in our A/B test. Dividing the difference between the point estimate and the null value by the standard error measures how far from normal the experiment data is. 

In order to measure this abnormality, we can look at a t distribution to get an idea of how unusual the observation is. Suppose the red vertical line is where our test statistic is on the x axis. In the image, you can see the shaded tail corresponds to the situations that would result in a test statistic more extreme than what was observed in our experiment. This area is called the p-value, and is the probability of observing results like what was seen in the experiment (or something more extreme) given that the null hypothesis is true. Thus, the smaller the p-value, the less likely the results occurred under the null and the stronger the evidence is in support of the alternative hypothesis.

Prior to the test, you select a cut-off value. If the p-value is less than the cut-off, then we conclude there is enough evidence to support the alternative hypothesis. This cut-off value is called the significance level. This value is typically 0.05, but can be adjusted to be more or less conservative.


---
## Conditions for Test

```yaml
type: "TwoColumns"
key: "f1abc8b757"
```

`@part1`
1. Independence {{1}}
   -  observations are not related {{2}}
   - randomly assign pop-up or not {{3}}
2. Nearly Normal {{4}}
   - data should be bell shaped {{5}}
   - watch out for skew {{6}}


`@part2`
![](http://assets.datacamp.com/production/repositories/3877/datasets/322c02c5d39407225ac7e3e3cac1b937ee71741c/hist.png){{5}}


`@script`
In order to conduct the test using the t-distribution as a comparison, a few conditions need to be met. First, we need independence. This means that the data in the two groups are not related. This condition is met if we randomly assign site visitors to the pop-up or no pop-up group.

Second, we need the data in each group to be nearly normal. When plotting a a histogram of the data, it should be somewhat bell-shaped. Skewed data will have a long tail in a single direction and will not meet this condition.


---
## Conducting the Test in Python

```yaml
type: "FullSlide"
key: "ebc1fde2b0"
```

`@part1`
```
from spicy import stats

test_stat, p_val = scipy.stats.ttest_ind(group_a, group_b)

print(test_stat)
print(p_val)
```


```
Test Statistic: -16.2622683309
P-value: 6.08651607765e-56
```


`@script`
Now that we've covered the basics of A/B testing, we're going to move on to how to conduct the test in Python. Once you've got your data loaded, it only takes a few lines of code. Here we are using a function from the scipy package. The first two arguments are the data from the two different versions of the trial. The group_a and group_b objects are just vectors containing all the observed times on site for each group. The third argument is whether or not the two groups have equal variances. The code returns the test statistic and the p-value.


---
## Final Slide

```yaml
type: "FinalSlide"
key: "fd755dd616"
```

`@script`
Knowing how to test if the difference between two means is significant opens up many possibilities for A/B testing in a business setting. You can use this technique in any situation where you want to compare the average value of a metric between two groups and the conditions for the test are met. You are now able to plan this type of test and execute it in Python.

While comparing means covers a lot situations you might want to test, there are other types of frequentist A/B tests that you are useful. In the next lesson, we will learn more about comparing proportions between two groups.

